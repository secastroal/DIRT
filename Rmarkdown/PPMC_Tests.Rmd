---
title: "Testing Posterior Predictive Model Checking for the Time-Varying Dynamic Partial Credit Model"
author: "Sebastian Castro-Alvarez"
date: "`r format(Sys.Date(), '%B %d del %Y')`"
header-includes:
  - \usepackage{booktabs}
  - \usepackage{longtable}
  - \usepackage{array}
  - \usepackage{multirow}
  - \usepackage{float}
  - \usepackage{amsmath}
output: 
  pdf_document:
    toc: false
bibliography: references.bib
csl: apa7.csl
link-citations: true
always_allow_html: true
---

```{r setup, include=FALSE}
library(knitr)
library(kableExtra)
knitr::opts_chunk$set(echo = FALSE, fig.height = 5, fig.width = 8,  
                      warning=FALSE, message=FALSE)

library(rstan)
rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())
library(bayesplot)
library("splines")

source("../R/IRT_models.R")
source("../R/IRT_plots.R")
source("../R/PPMC.R")
source("../R/genTVDPCM.R")
source("../R/tvdpcm2stan.R")
```

```{r seed}
seed <- 1005
set.seed(seed)
```

```{r setmodel}
# Use TVPCM, ARPCM, or PCM
setmodel <- "TVPCM"
# render("Rmarkdown/PPMC_Tests.Rmd", output_file = paste0("Rmarkdown/PPMC_Tests_", setmodel, "_", seed))
```

In this document, we test the posterior predictive model checking methods that we are developing for the time-varying dynamic partial credit model (TV-DPCM). For this, we simulated data based on the TV-DPCM as well as data that violates some of the assumptions of the model.

# TV-DPCM

The time-varying dynamic partial credit model is a model that aims to analyze the psychological time series of one individual. Consider for example that a person that is attending therapy is requested to report their emotions multiple times on a daily basis to monitor their progress. Thus, the person has to answer multiple likert-scale questions (the same questions every time) about how they feel. Moreover, we assume that some of these emotions measure a construct such as positive or negative affect. Given this situation, the observed data are categorical time series from a single individual. To analyze such data, we are developing (work still in progress) the time-varying dynamic partial credit model. 

This model combines two frameworks: The item response theory [@Embretson2000] and the time-varying vector autoregressive model [@Bringmann2017]. In particular, the measurement component of the model uses the partial credit model [@Masters2016] to relate the responses to the items with the latent construct of interest. Then, at the latent label, the latent variable, which represents the latent state dispositions of the individual at each time point, is modeled by means of a time-varying autoregressive model [@Bringmann2017]. In this case, we only allow the intercept of the autoregressive model to vary over time while the autoregressive effect is assumed to be time invariant. Thus, the intercept is further modeled with the generalized additive model with splines. This allows the time series to follow a non linear (smooth) trend.

# Data Simulation

In this example, we simulated four data sets. The first data set perfectly matches the proposed TV-DPCM. The second data set is a constrained version of the TV-DPCM model in which the intercept does not vary over time. A third data set assumes that the latent structure is bidimensional instead of unidimensional. The last data set assumes that there is item parameter drift (i.e., the item parameters change over time). For these data sets, we simulated 200 time points of 6 items each with 5 response options. Moreover, the autoregressive effect is set at 0.5 and the variance of the innovations is set to 1.  

```{r fixvalues}
nT     <- 200   # Number of time points
I      <- 6     # Number of items
K      <- 5     # Number of categories per item
M      <- K - 1 # Number of thresholds per item
lambda <- 0.5   # Size of the autoregressive effect
in_var <- 1     # Variance of the innovations
```

Data1 perfectly matches the TV-DPCM. In this case, we simulate the time-varying intercept based on a B-splines of degree three with 2 internal knots. The simulated time series of the sumscores are presented in following figure: 

```{r data1}
# Generate Data based on the TV-DPCM
dataTRUE <- gen.TVDPCM(nT = nT, 
                       I  = I, 
                       K  = K, 
                       pop.param = list(lambda = lambda), 
                       seed = seed, 
                       FUN  = "sinusoidal",
                       maxAbsValue = 1)

data1 <- dataTRUE$data
```

```{r data1_plot}
plot(rowSums(data1), type = "l", ylab = "sumscores", xlab = "Time",
     las = 1, lwd = 2)
```

Secondly, Data2 was simulated based on a simpler version of the model that does not allow the intercept to vary over time. In this case, the latent variable is also modeled after an autoregressive model, but the structure is assumed to be stationary. Therefore, there are not trends in the time series. In this case, the simulated time series of the sumscores look like this:

```{r data2}
# Simulate data with no trend
data2 <- gen.TVDPCM(nT = nT,
                    I  = I,
                    K  = K,
                    pop.param = list(lambda = dataTRUE$lambda.gen,
                                     sigma2 = dataTRUE$sigma2.gen,
                                     thresholds = dataTRUE$thresholds.gen),
                    seed = seed,
                    FUN = function(x) {rep(0, nT)})

data2 <- data2$data
```

```{r data2_plot}
plot(rowSums(data2), type = "l", ylab = "sumscores", xlab = "Time",
     las = 1, lwd = 2)
```

The next data set (Data3) assumes that the factor structure is bidimensional instead of unidimensional. The two factors are correlated ($r = 0.6$). The latent process of these two factors was also simulated based on an autoregressive model without time varying parameters. Thus, the simulated time series of the sumscores of each factor and the whole scale are presented in the following figure:

```{r data3}
# Now, theta is a matrix with two factors that are weakly correlated.
theta <- matrix(NA, nrow = nT, ncol = 2)
mu    <- c(0, 0)
Sigma <- matrix(c(1, 0.3, 0.3, 1), 2)

theta[1, ] <- MASS::mvrnorm(1, mu = mu, Sigma = Sigma)

for (i in 2:nT) {
  theta[i, ] <- lambda * theta[i - 1, ] + MASS::mvrnorm(1, mu = mu, Sigma = Sigma) 
}
rm(i, mu, Sigma)

# Now, let's use the same item parameters to generate the data but using three
# items for one factor and three items for the other factor.
# Location
delta <- rowMeans(dataTRUE$thresholds.gen)
  
# Steps
taus  <- dataTRUE$thresholds.gen - delta

# Responses factor 1
probs.array <- array(NA, dim = c(dim(theta)[1], I / 2, K))

for (y in 0:M) {
  probs.array[, , y + 1] <- P.GPCM(y     = y, 
                                   alpha = rep(1, I / 2), 
                                   delta = delta[1:3], 
                                   taus  = taus[1:3, ], 
                                   theta = theta[, 1], 
                                   M     = M)
}
responses1   <- apply(probs.array, 1:2, function(vec) {which( rmultinom(1, 1, vec) == 1) - 1 })
responses1   <- responses1 + 1 # To fit the PCM in stan, items should be coded starting from 1. 
rm(probs.array, y)

# Responses factor 2
probs.array <- array(NA, dim = c(dim(theta)[1], I / 2, K))

for (y in 0:M) {
  probs.array[, , y + 1] <- P.GPCM(y     = y, 
                                   alpha = rep(1, I / 2), 
                                   delta = delta[4:6], 
                                   taus  = taus[4:6, ], 
                                   theta = theta[, 2], 
                                   M     = M)
}
responses2   <- apply(probs.array, 1:2, function(vec) {which( rmultinom(1, 1, vec) == 1) - 1 })
responses2   <- responses2 + 1 # To fit the PCM in stan, items should be coded starting from 1. 
rm(probs.array, y)

responses <- cbind(responses1, responses2)
rm(responses1, responses2)

data3 <- responses
```

```{r data3_plot}
plot(rowSums(data3), type = "l", ylab = "sumscores", xlab = "Time",
     las = 1, ylim = c(2.75, 30.25), lwd = 2)
lines(rowSums(data3[, 1:3]), col = "blue", lwd = 2)
lines(rowSums(data3[, 4:6]), col = "darkgreen", lwd = 2)
legend("topright", c("Total", "Factor 1", "Factor 2"),
       col = c("black", "blue", "darkgreen"), lty = 1, lwd = 2)

```

Lastly, Data4 assumes that the item parameters change over time. For this, we keep the same generated thetas as the ones use to generate Data1. We also use the same item parameters as we used for the other data sets to generate the first half of the responses. In contrast, for the second half of the responses, the thresholds parameters are modified by resting 2 to all of them. This means that the individual is more likely to endorse high responses on the second half of the measurement. Now, the following figure presents the simulated time series:

```{r data4}
# Use same theta as in data1
theta <- dataTRUE$theta.gen

# Create item parameters for the second half.
thresholds2 <- dataTRUE$thresholds.gen - 1.5

# Location
delta2 <- rowMeans(thresholds2)

# Step parameters
taus2 <- thresholds2 - delta2

# Generate responses  half 1
probs.array <- array(NA, dim = c(length(theta)/2, I, K))

for (y in 0:M) {
  probs.array[, , y + 1] <- P.GPCM(y     = y, 
                                   alpha = rep(1, I), 
                                   delta = delta, 
                                   taus  = taus, 
                                   theta = theta[1:100], 
                                   M     = M)
}
responses1   <- apply(probs.array, 1:2, function(vec) {which( rmultinom(1, 1, vec) == 1) - 1 })
responses1   <- responses1 + 1 # To fit the PCM in stan, items should be coded starting from 1. 
rm(probs.array, y)

# Generate responses  half 2
probs.array <- array(NA, dim = c(length(theta)/2, I, K))

for (y in 0:M) {
  probs.array[, , y + 1] <- P.GPCM(y     = y, 
                                   alpha = rep(1, I), 
                                   delta = delta2, 
                                   taus  = taus2, 
                                   theta = theta[101:200], 
                                   M     = M)
}
responses2   <- apply(probs.array, 1:2, function(vec) {which( rmultinom(1, 1, vec) == 1) - 1 })
responses2   <- responses2 + 1 # To fit the PCM in stan, items should be coded starting from 1. 
rm(probs.array, y)

responses <- rbind(responses1, responses2)
rm(responses1, responses2)

data4 <- responses
```

```{r data4_plot}
plot(rowSums(data4[1:100, ]), type = "l", ylab = "sumscores", xlab = "Time",
     las = 1, xlim = c(1, 200), lwd = 2, col = "blue")
lines(100:200, rowSums(data4[100:200, ]), col = "darkgreen", lwd = 2)
```

# Posterior Predictive Model Checking Methods

In this section, we fit the TV-DPCM to all the generated data sets. Then, we compute some of the posterior predictive checking methods and modified versions of these methods that have been proposed for IRT in @Li2017, @Sinharay2006a, and @Zhu2011. First, let's verify by means of the Rubin-Gelman statistics that the model converged on each of the data sets. This is presented in the following figure: 

```{r selectmodel}
if (setmodel == "TVPCM") {
  modelfile   <- "tv_dpcm_int_v5.1.stan"
  paramsample <- c("beta", "theta", "lambda", "sigma2", "pvar", 
                   "attractor", "rep_y")
  paramplot   <- c("beta[3,3]", "theta[100]", "attractor[50]", 
                   "lambda", "sigma2", "pvar")
  inits <- function() {
  list(lambda = runif(1, -1, 1),
       beta   = array(rnorm(I * (K - 1), 0, 3), dim = c(I, K - 1)),
       inno   = rnorm(nT, 0, 3),
       sigma  = rlnorm(1, 1))
  }
}

if (setmodel == "ARPCM") {
  modelfile   <- "ar_irt_pcm_na_free.stan"
  paramsample <- c("beta", "theta", "lambda", "sigma2", "rep_y")
  paramplot   <- c("beta[1,1]", "beta[3,3]", "beta[6,4]", "theta[1]", 
                   "theta[100]", "theta[200]", "lambda", "sigma2")
  inits <- function() {
  list(lambda = runif(1, -1, 1),
       beta   = array(rnorm(I * (K - 1), 0, 3), dim = c(I, K - 1)),
       inno   = rnorm(nT, 0, 3),
       sigma2 = rlnorm(1, 1))
  }
}

if (setmodel == "PCM") {
  modelfile   <- "irt_pcm_long.stan"
  paramsample <- c("beta", "theta", "rep_y")
  paramplot   <- c("beta[1,1]", "beta[3,3]", "beta[6,4]", "theta[1]", 
                   "theta[100]", "theta[200]")
  inits <- function() {
  list(beta   = array(rnorm(I * (K - 1), 0, 3), dim = c(I, K - 1)),
       theta   = rnorm(nT, 0, 3))
  }
}

```


```{r stanfit}
model <- stan_model(file = paste0("../Stan/", modelfile), verbose = FALSE)

datasets <- list(data1, data2, data3, data4)
standata <- list()
fit      <- list()

# Fit the TV-DPCM model with stan
for (i in 1:4) {

responses <- datasets[[i]]

standata[[i]] <- tvdpcm2stan_data(resp = responses,
                                  I = I,
                                  K = K,
                                  nT = nT,
                                  n_knots = 8,
                                  s_degree = 3)
            
begin.time <- proc.time()
fit[[i]] <- sampling(model,                            # Stan model. 
                     data = standata[[i]],             # Data.
                     iter = 2000,                      # Number of iterations.
                     chains  = 3,                      # Number of chains.
                     warmup  = 500,                   # Burn-in samples.
                     init    = inits,
                     seed    = seed,
                     pars    = paramsample,
                     control = list(adapt_delta=0.99, max_treedepth = 15)) # Other parameters to control sampling behavior.
run.time <- proc.time() - begin.time
rm(begin.time)
            
}
```

## Verify model convergence.

```{r diagnostics, results='hide'}

diag <- lapply(fit, function (x) {
  monitor(extract(x, permuted = FALSE, inc_warmup = FALSE), warmup = 0)
})

diag.table <- matrix(NA, nrow = 6, ncol = 4)
colnames(diag.table)  <- paste0("Data", 1:4)
row.names(diag.table) <- c("Rhat>1.05", "N. Divergent", "N. Low BFMI", 
                           "Excedeed Treedepth", "Low Bulk ESS", "Low Tail ESS")

diag.table[1, ] <- unlist(
  lapply(fit, function(x) {
    sum(rhat(x, pars = head(paramsample, -1)) > 1.05)
    }))

diag.table[2, ] <- unlist(
  lapply(fit, get_num_divergent)
)

diag.table[3, ] <- unlist(
  lapply(fit, function(x) length(get_low_bfmi_chains(x)))
)

diag.table[4, ] <- unlist(
  lapply(fit, get_num_max_treedepth)
)

diag.table[5, ] <- unlist(
  lapply(diag, function(x) sum(x$Bulk_ESS < 300))
)

diag.table[6, ] <- unlist(
  lapply(diag, function(x) sum(x$Tail_ESS < 300))
)
```

```{r diag_table}
kbl(diag.table, align = "c", booktabs = TRUE)
```

```{r pairs_plot}
for (i in 1:4) {
  pairs(fit[[i]], pars = paramplot, log = TRUE, las = 1)
}
rm(i)
```

```{r rhat_plot, fig.height = 8}
par(mfrow = c(2, 2))
for (i in 1:4) {
  print(mcmc_rhat(rhat(fit[[i]])))
}
rm(i)
```

```{r trace_plot}
for (i in 1:4) {
  print(traceplot(fit[[i]], pars = paramplot, 
                  inc_warmup = FALSE))
}
rm(i)
```

## Sumscores Time Series

```{r sums_ts, fig.height = 8}
par(mfrow = c(4, 1))
for(i in 1:4) {
  ppmc.sumscore.ts(fit[[i]], standata[[i]])
}
rm(i)
```

## Autocorrelation: 1st, 2nd, and 3rd

```{r acf, fig.height = 8}
par(mfrow = c(4, 3))
for(i in 1:4) {
  ppmc.acf(fit[[i]], standata[[i]], lag.max = 3)
}
rm(i)
```

## Autocorrelation of the Residuals: 1st

```{r racf, fig.height = 8}
par(mfrow = c(2, 2))
for(i in 1:4) {
  ppmc.racf(fit[[i]], standata[[i]])
}
rm(i)
```

## Mean Squared Succesive Differences

```{r racf, fig.height = 8}
par(mfrow = c(2, 2))
for(i in 1:4) {
  ppmc.mssd(fit[[i]], standata[[i]])
}
rm(i)
```


## Items Scores Time Series: Item 1

```{r item_ts, fig.height = 8}
par(mfrow = c(4, 1))
for(i in 1:4) {
  ppmc.item.ts(fit[[i]], standata[[i]], quiet = TRUE, items = 1)
}
rm(i)
```

## Item-Total Correlation: Version 1
Data1:
```{r itcor_1, fig.height = 6, results='hide'}
par(mfrow = c(2, 3))
ppmc.itcor(fit[[1]], standata[[1]], quiet = TRUE, method = "pearson")
```
Data2:
```{r itcor_2, fig.height = 6, results='hide'}
par(mfrow = c(2, 3))
ppmc.itcor(fit[[2]], standata[[2]], quiet = TRUE, method = "pearson")
```
Data3:
```{r itcor_3, fig.height = 6, results='hide'}
par(mfrow = c(2, 3))
ppmc.itcor(fit[[3]], standata[[3]], quiet = TRUE, method = "pearson")
```
Data4:
```{r itcor_4, fig.height = 6, results='hide'}
par(mfrow = c(2, 3))
ppmc.itcor(fit[[4]], standata[[4]], quiet = TRUE, method = "pearson")
```

## Item-Total Correlation: Version 2
Data1:
```{r itcor2_1, fig.height = 6, results='hide'}
par(mfrow = c(2, 3))
ppmc.itcor2(fit[[1]], standata[[1]], quiet = TRUE, method = "polyserial")
```
Data2:
```{r itcor2_2, fig.height = 6, results='hide'}
par(mfrow = c(2, 3))
ppmc.itcor2(fit[[2]], standata[[2]], quiet = TRUE, method = "polyserial")
```
Data3:
```{r itcor2_3, fig.height = 6, results='hide'}
par(mfrow = c(2, 3))
ppmc.itcor2(fit[[3]], standata[[3]], quiet = TRUE, method = "polyserial")
```
Data4:
```{r itcor2_4, fig.height = 6, results='hide'}
par(mfrow = c(2, 3))
ppmc.itcor2(fit[[4]], standata[[4]], quiet = TRUE, method = "polyserial")
```


## Item-Total Correlation: Version 3
Data1:
```{r itcor3_1, fig.height = 6, results='hide'}
par(mfrow = c(2, 3))
ppmc.itcor3(fit[[1]], standata[[1]], quiet = TRUE)
```
Data2:
```{r itcor3_2, fig.height = 6, results='hide'}
par(mfrow = c(2, 3))
ppmc.itcor3(fit[[2]], standata[[2]], quiet = TRUE)
```
Data3:
```{r itcor3_3, fig.height = 6, results='hide'}
par(mfrow = c(2, 3))
ppmc.itcor3(fit[[3]], standata[[3]], quiet = TRUE)
```
Data4:
```{r itcor3_4, fig.height = 6, results='hide'}
par(mfrow = c(2, 3))
ppmc.itcor3(fit[[4]], standata[[4]], quiet = TRUE)
```

## Yen's Q1

Data1:
```{r q1_1, fig.height = 6, results='hide'}
par(mfrow = c(2, 3))
ppmc.Q1(fit[[1]], standata[[1]], quiet = TRUE)
```

Data2:
```{r q1_2, fig.height = 6, results='hide'}
par(mfrow = c(2, 3))
ppmc.Q1(fit[[2]], standata[[2]], quiet = TRUE)
```

Data3:
```{r q1_3, fig.height = 6, results='hide'}
par(mfrow = c(2, 3))
ppmc.Q1(fit[[3]], standata[[3]], quiet = TRUE)
```

Data4:
```{r q1_4, fig.height = 6, results='hide'}
par(mfrow = c(2, 3))
ppmc.Q1(fit[[4]], standata[[4]], quiet = TRUE)
```

## Yen's Q1 modified

Data1:
```{r q1alt_1, fig.height = 6, results='hide'}
par(mfrow = c(2, 3))
ppmc.Q1.alt(fit[[1]], standata[[1]], quiet = TRUE)
```

Data1:
```{r q1alt_2, fig.height = 6, results='hide'}
par(mfrow = c(2, 3))
ppmc.Q1.alt(fit[[2]], standata[[2]], quiet = TRUE)
```

Data1:
```{r q1alt_3, fig.height = 6, results='hide'}
par(mfrow = c(2, 3))
ppmc.Q1.alt(fit[[3]], standata[[3]], quiet = TRUE)
```

Data4:
```{r q1alt_4, fig.height = 6, results='hide'}
par(mfrow = c(2, 3))
ppmc.Q1.alt(fit[[4]], standata[[4]], quiet = TRUE)
```


# References
