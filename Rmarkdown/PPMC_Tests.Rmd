---
title: "Testing Posterior Predictive Model Checking for the Time-Varying Dynamic Partial Credit Model"
author: "Sebastian Castro-Alvarez"
date: "`r format(Sys.Date(), '%B %d del %Y')`"
header-includes:
  - \usepackage{booktabs}
  - \usepackage{longtable}
  - \usepackage{array}
  - \usepackage{multirow}
  - \usepackage{float}
  - \usepackage{amsmath}
output: 
  pdf_document:
    toc: false
params:
  seed: 1234
  setmodel: "TVPCM"
bibliography: references.bib
csl: apa7.csl
link-citations: true
always_allow_html: true
---

```{r setup, include=FALSE}
library(knitr)
library(kableExtra)
knitr::opts_chunk$set(echo = FALSE, fig.height = 5, fig.width = 8,  
                      warning=FALSE, message=FALSE)

library(rstan)
rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores() - 1)
library(bayesplot)

source("../R/IRT_models.R")
source("../R/IRT_plots.R")
source("../R/PPMC.R")
source("../R/genTVDPCM.R")
source("../R/tvdpcm2stan.R")
```

```{r seed}
set.seed(params$seed)
```

```{r setmodel}
# Use TVPCM, ARPCM, or PCM
setmodel <- params$setmodel

# seeds <- 1002:1005
# setmodel <- "TVPCM"
# for (i in 1:length(seeds)) {
#   seed <- seeds[i]
#   rmarkdown::render("Rmarkdown/PPMC_Tests.Rmd", params = list(seed = seed), output_file = paste0("PPMC_Tests_", setmodel, "_", seed))
# }
```

In this document, we test the posterior predictive model checking methods that we are developing for the time-varying dynamic partial credit model (TV-DPCM). For this, we simulated data based on the TV-DPCM as well as data that violates some of the assumptions of the model.

# TV-DPCM

The time-varying dynamic partial credit model is a model that aims to analyze the psychological time series of one individual. Consider for example that a person that is attending therapy is requested to report their emotions multiple times on a daily basis to monitor their progress. Thus, the person has to answer multiple likert-scale questions (the same questions every time) about how they feel. Moreover, we assume that some of these emotions measure a construct such as positive or negative affect. Given this situation, the observed data are categorical time series from a single individual. To analyze such data, we developed the time-varying dynamic partial credit model. 

This model combines two frameworks: The item response theory [@Embretson2000] and the time-varying vector autoregressive model [@Bringmann2017]. In particular, the measurement component of the model uses the partial credit model [@Masters2016] to relate the responses to the items with the latent construct of interest. Then, at the latent level, the latent variable, which represents the latent state dispositions of the individual at each time point, is modeled by means of a time-varying autoregressive model [@Bringmann2017]. In this case, we only allow the intercept of the autoregressive model to vary over time while the autoregressive effect is assumed to be time invariant. Thus, the intercept is further modeled with the generalized additive model with splines [@Wood2017]. This allows the time series to follow a non linear (smooth) trend. To be more precise, the model is defined in two equations: The measurement equation and the structural equation. The measurement equation defines the measurement model that relates the responses with the latent construct, namely, the PCM, which is as follows:

$$
P(X_i = x|\theta_t) = \frac{exp\Big[\sum\limits_{k=0}^{x}(\theta_t - \delta_{ik})\Big]}{\sum\limits_{v=0}^{m_{i}}exp\Big[\sum\limits_{k=0}^{v}(\theta_t - \delta_{ik})\Big]},
$$

where $\theta_{t}$ is the latent state disposition at time $t$ and $\delta_{ik}$ is the step parameter of the $k$-th category of the $i$-th item. Then, the latent state dispositions ($\theta_t$) are further modeled at the structural equation with the TV-AR model. This means that $\theta_t$ is regressed on a lagged version of itself, resulting in the following equation:

$$
\theta_{t} = \alpha_{t} + \varphi\theta_{t-1} + \varepsilon_{t},
$$

where $\alpha_t$ is the time varying intercept of the dynamic process, $\varphi$ is the time invariant autoregressive effect between consecutive latent state dispositions, and $\varepsilon_{t}$ is the residual, also known as the innovation, at time $t$. Lastly, the time varying intercepts ($\alpha_t$) are further modeled as a function of time with a generalized additive model. This means that:

$$
\alpha_{t} = f(t) = \sum\limits_{j=1}^{s}b_{j}(t)\beta_{j},
$$

where $b_j(t)$ is a function of time, which is known as a basis function. These basis functions can be specified in many different ways given a certain smoother [@Wood2017]. In this particular implementation, we define the basis functions based on B-splines.

\newpage

# Data Simulation

In this example, we simulated six data sets. The first data set perfectly matches the proposed TV-DPCM. The second data set is a constrained version of the TV-DPCM model in which the intercept does not vary over time. A third data set assumes that the latent structure is bidimensional instead of unidimensional. The fourth data set assumes that there is item parameter drift (i.e., the item parameters change over time). The fifth data set simulates data based on the generalized PCM, in other words, it allows the discrimination parameters to vary across items. Lastly, the last data set simulates a TV-DPCM of order 3 (i.e., the latent state disposition is regressed on the three previous time points). For these data sets, we simulated 200 time points of 6 items each with 5 response options. Moreover, the autoregressive effect is set at 0.5 and the variance of the innovations is set to 1.  

```{r fixvalues}
nT     <- 200   # Number of time points
I      <- 6     # Number of items
K      <- 5     # Number of categories per item
M      <- K - 1 # Number of thresholds per item
lambda <- 0.5   # Size of the autoregressive effect
in_var <- 1     # Variance of the innovations
```

Data1 perfectly matches the TV-DPCM. In this case, we simulate the time-varying intercept based on sinusoidal trend with one and a half period. The simulated time series of the sumscores are presented in following figure: 

```{r data1}
# Generate Data based on the TV-DPCM
dataTRUE <- gen.TVDPCM(nT = nT, 
                       I  = I, 
                       K  = K, 
                       pop.param = list(lambda = lambda), 
                       seed = seed, 
                       FUN  = "sinusoidal",
                       maxAbsValue = 1)

data1 <- dataTRUE$data
```

```{r data1_plot}
plot(rowSums(data1), type = "l", ylab = "sumscores", xlab = "Time",
     las = 1, lwd = 2)
```

Secondly, Data2 was simulated based on a simpler version of the model that does not allow the intercept to vary over time. In this case, the latent variable is also modeled after an autoregressive model, but the structure is assumed to be stationary. Therefore, there are not trends in the time series. In this case, the simulated time series of the sumscores look like this:

```{r data2}
# Simulate data with no trend
data2 <- gen.TVDPCM(nT = nT,
                    I  = I,
                    K  = K,
                    pop.param = list(lambda = dataTRUE$lambda.gen,
                                     sigma2 = dataTRUE$sigma2.gen,
                                     thresholds = dataTRUE$thresholds.gen),
                    seed = seed,
                    FUN = function(x) {rep(0, nT)})

data2 <- data2$data
```

```{r data2_plot}
plot(rowSums(data2), type = "l", ylab = "sumscores", xlab = "Time",
     las = 1, lwd = 2)
```

The next data set (Data3) assumes that the factor structure is bidimensional instead of unidimensional. The two factors are correlated ($r = 0.5$). The latent process of these two factors was also simulated based on an autoregressive model without time varying parameters. The first three items are one factor and the other three are the other factor. Thus, the simulated time series of the sumscores of each factor and the whole scale are presented in the following figure:

```{r data3}
# Now, theta is a matrix with two factors that are weakly correlated.
theta <- matrix(NA, nrow = nT, ncol = 2)
mu    <- c(0, 0)
Sigma <- matrix(c(1, 0.5, 0.5, 1), 2)

theta[1, ] <- MASS::mvrnorm(1, mu = mu, Sigma = Sigma)

for (i in 2:nT) {
  theta[i, ] <- lambda * theta[i - 1, ] + MASS::mvrnorm(1, mu = mu, Sigma = Sigma) 
}
rm(i, mu, Sigma)

# Now, let's use the same item parameters to generate the data but using three
# items for one factor and three items for the other factor.
# Location
delta <- rowMeans(dataTRUE$thresholds.gen)
  
# Steps
taus  <- dataTRUE$thresholds.gen - delta

# Responses factor 1
probs.array <- array(NA, dim = c(dim(theta)[1], I / 2, K))

for (y in 0:M) {
  probs.array[, , y + 1] <- P.GPCM(y     = y, 
                                   alpha = rep(1, I / 2), 
                                   delta = delta[1:3], 
                                   taus  = taus[1:3, ], 
                                   theta = theta[, 1], 
                                   M     = M)
}
responses1   <- apply(probs.array, 1:2, function(vec) {which( rmultinom(1, 1, vec) == 1) - 1 })
responses1   <- responses1 + 1 # To fit the PCM in stan, items should be coded starting from 1. 
rm(probs.array, y)

# Responses factor 2
probs.array <- array(NA, dim = c(dim(theta)[1], I / 2, K))

for (y in 0:M) {
  probs.array[, , y + 1] <- P.GPCM(y     = y, 
                                   alpha = rep(1, I / 2), 
                                   delta = delta[4:6], 
                                   taus  = taus[4:6, ], 
                                   theta = theta[, 2], 
                                   M     = M)
}
responses2   <- apply(probs.array, 1:2, function(vec) {which( rmultinom(1, 1, vec) == 1) - 1 })
responses2   <- responses2 + 1 # To fit the PCM in stan, items should be coded starting from 1. 
rm(probs.array, y)

responses <- cbind(responses1, responses2)
rm(responses1, responses2)

data3 <- responses
```

```{r data3_plot}
plot(rowSums(data3), type = "l", ylab = "sumscores", xlab = "Time",
     las = 1, ylim = c(2.75, 30.25), lwd = 2)
lines(rowSums(data3[, 1:3]), col = "blue", lwd = 2)
lines(rowSums(data3[, 4:6]), col = "darkgreen", lwd = 2)
legend("topright", c("Total", "Factor 1", "Factor 2"),
       col = c("black", "blue", "darkgreen"), lty = 1, lwd = 2)

```

Next, Data4 assumes that the item parameters change over time. For this, we keep the same generated thetas as the ones use to generate Data1. We also use the same item parameters as we used for the other data sets to generate the first half of the responses. In contrast, for the second half of the responses, the thresholds parameters are modified by resting 1.5 to all of them. This means that the individual is more likely to endorse high responses on the second half of the measurement. Now, the following figure presents the simulated time series:

```{r data4}
# Use same theta as in data1
theta <- dataTRUE$theta.gen

# Create item parameters for the second half.
thresholds2 <- dataTRUE$thresholds.gen - 1.5

# Location
delta2 <- rowMeans(thresholds2)

# Step parameters
taus2 <- thresholds2 - delta2

# Generate responses  half 1
probs.array <- array(NA, dim = c(length(theta)/2, I, K))

for (y in 0:M) {
  probs.array[, , y + 1] <- P.GPCM(y     = y, 
                                   alpha = rep(1, I), 
                                   delta = delta, 
                                   taus  = taus, 
                                   theta = theta[1:100], 
                                   M     = M)
}
responses1   <- apply(probs.array, 1:2, function(vec) {which( rmultinom(1, 1, vec) == 1) - 1 })
responses1   <- responses1 + 1 # To fit the PCM in stan, items should be coded starting from 1. 
rm(probs.array, y)

# Generate responses  half 2
probs.array <- array(NA, dim = c(length(theta)/2, I, K))

for (y in 0:M) {
  probs.array[, , y + 1] <- P.GPCM(y     = y, 
                                   alpha = rep(1, I), 
                                   delta = delta2, 
                                   taus  = taus2, 
                                   theta = theta[101:200], 
                                   M     = M)
}
responses2   <- apply(probs.array, 1:2, function(vec) {which( rmultinom(1, 1, vec) == 1) - 1 })
responses2   <- responses2 + 1 # To fit the PCM in stan, items should be coded starting from 1. 
rm(probs.array, y)

responses <- rbind(responses1, responses2)
rm(responses1, responses2)

data4 <- responses
```

```{r data4_plot}
plot(rowSums(data4[1:100, ]), type = "l", ylab = "sumscores", xlab = "Time",
     las = 1, xlim = c(1, 200), lwd = 2, col = "blue")
lines(100:200, rowSums(data4[100:200, ]), col = "darkgreen", lwd = 2)
```

The fifth dataset (Data5) simulates data using the GPCM instead of the PCM for the measurement equation. This means that the discrimination parameters vary across items. The discrimination parameters are 1 for items 1 to 4, 0.5 for item 5 and 1.5 for item 6. The simulated data is presented in following Figure:

```{r data5}
# Generate Data based on the TV-DPCM using the GPCM for the measurement model.
data5 <- gen.TVDPCM(nT = nT, 
                    I  = I, 
                    K  = K, 
                    pop.param = list(lambda = dataTRUE$lambda.gen,
                                     sigma2 = dataTRUE$sigma2.gen,
                                     thresholds = dataTRUE$thresholds.gen,
                                     theta = dataTRUE$theta.gen,
                                     alpha = c(1, 1, 1, 1, 0.5, 1.5)), 
                    seed = seed, 
                    FUN  = "sinusoidal",
                    maxAbsValue = 1)

alpha <- data5$alpha.gen
data5 <- data5$data
```

```{r data5_plot}
plot(rowSums(data5), type = "l", ylab = "sumscores", xlab = "Time",
     las = 1, lwd = 2)
```

Finally, the last data set (Data6) is simulated based on a dynamic process of order 3. This means that the latent state disposition at time $t$ has an effect on the latent state disposition at time $t + 3$. The observed time series of Data6 are presented in the following Figure:

```{r data6}
# Generate the dynamic process of order 3.
# Define autoregressive effect up to lag 3.
lambda <- c(0.3, 0.2, 0.1)

# Use the same time varying intercept as in Data1.
tv_int <- sinusoidal(nT, 1)
tv_int <- c(tv_int * -1, tv_int)

# We generate a dynamic process twice the required length and the drop the first 
# half
theta <- rep(NA, nT * 2)

# The three first thetas are randomly generated.
theta[1:3] <- rnorm(1, 0, sqrt(dataTRUE$sigma2.gen))

for (t in 4:(nT * 2)) {
      theta[t] <- tv_int[t] + lambda[1] * theta[t - 1] +
        lambda[2] * theta[t - 2] + lambda[3] * theta[t - 3] +
        rnorm(1, 0, sqrt(dataTRUE$sigma2.gen))
    }
    rm(t)

theta <- theta[(nT + 1):(nT * 2)]

# Generate Data based on the TV-DPCM
data6 <- gen.TVDPCM(nT = nT, 
                    I  = I, 
                    K  = K, 
                    pop.param = list(lambda = lambda[1],
                                     sigma2 = dataTRUE$sigma2.gen,
                                     thresholds = dataTRUE$thresholds.gen,
                                     theta = theta), 
                    seed = seed, 
                    FUN  = "sinusoidal",
                    maxAbsValue = 1)

data6 <- data6$data
```

```{r data6_plot}
plot(rowSums(data6), type = "l", ylab = "sumscores", xlab = "Time",
     las = 1, lwd = 2)
```

In the next section, we proceed to analyze the simulated data sets with the TV-DPCM and to compute several posterior predictive model checking methods.

\newpage

# Posterior Predictive Model Checking Methods

In this section, we fit the TV-DPCM to all the generated data sets. To run the algorithm, we used 3 parallel chains and 2000 iterations per chain, of which 500 were discarded as burn-in. For the B-splines, we used 8 internal knots, which means that we used 10 basis functions to model the trend. Once the estimation of the model was finished, we computed some of the posterior predictive model checking methods and modified versions of these methods that have been proposed for IRT in @Li2017, @Sinharay2006a, and @Zhu2011. First, to verify that the models converged, we looked at the warning messages from Stan. These messages indicate if there were divergent transitions, if there were Gelman-Rubin statistics larger than 1.05, and other checks available in Stan. These diagnostic checks are presented in the following Table for each data set. If the estimation of the model finished without any issues, all the different warnings must be 0. 

```{r selectmodel}
if (setmodel == "TVPCM") {
  modelfile   <- "tv_dpcm_int_v5.1.stan"
  paramsample <- c("beta", "theta", "lambda", "sigma2", "pvar", 
                   "attractor", "rep_y")
  paramplot   <- c("beta[3,3]", "theta[100]", "attractor[50]", 
                   "lambda", "sigma2", "pvar")
  inits <- function() {
  list(lambda = runif(1, -1, 1),
       beta   = array(rnorm(I * (K - 1), 0, 3), dim = c(I, K - 1)),
       inno   = rnorm(nT, 0, 3),
       sigma  = rlnorm(1, 1))
  }
}

if (setmodel == "ARPCM") {
  modelfile   <- "ar_irt_pcm_na_free.stan"
  paramsample <- c("beta", "theta", "lambda", "sigma2", "rep_y")
  paramplot   <- c("beta[1,1]", "beta[3,3]", "beta[6,4]", "theta[1]", 
                   "theta[100]", "theta[200]", "lambda", "sigma2")
  inits <- function() {
  list(lambda = runif(1, -1, 1),
       beta   = array(rnorm(I * (K - 1), 0, 3), dim = c(I, K - 1)),
       inno   = rnorm(nT, 0, 3),
       sigma2 = rlnorm(1, 1))
  }
}

if (setmodel == "PCM") {
  modelfile   <- "irt_pcm_long.stan"
  paramsample <- c("beta", "theta", "rep_y")
  paramplot   <- c("beta[1,1]", "beta[3,3]", "beta[6,4]", "theta[1]", 
                   "theta[100]", "theta[200]")
  inits <- function() {
  list(beta   = array(rnorm(I * (K - 1), 0, 3), dim = c(I, K - 1)),
       theta   = rnorm(nT, 0, 3))
  }
}

```


```{r stanfit, eval = !(exists("standata") & exists("fit"))}
model <- stan_model(file = paste0("../Stan/", modelfile), verbose = FALSE)

datasets <- list(data1, data2, data3, data4, data5, data6)
standata <- list()
fit      <- list()

# Fit the TV-DPCM model with stan
for (i in 1:6) {

responses <- datasets[[i]]

standata[[i]] <- tvdpcm2stan_data(resp = responses,
                                  I = I,
                                  K = K,
                                  nT = nT,
                                  n_knots = 8,
                                  s_degree = 3)
            
begin.time <- proc.time()
fit[[i]] <- sampling(model,                            # Stan model. 
                     data = standata[[i]],             # Data.
                     iter = 2000,                      # Number of iterations.
                     chains  = 3,                      # Number of chains.
                     warmup  = 500,                   # Burn-in samples.
                     init    = inits,
                     seed    = seed,
                     pars    = paramsample,
                     control = list(adapt_delta=0.99, max_treedepth = 15)) # Other parameters to control sampling behavior.
run.time <- proc.time() - begin.time
rm(begin.time)
            
}
```

```{r diagnostics, results='hide'}

diag <- lapply(fit, function (x) {
  monitor(extract(x, pars = head(paramsample, -1), 
                  permuted = FALSE, inc_warmup = FALSE), 
          warmup = 0)
})

diag.table <- matrix(NA, nrow = 6, ncol = 6)
row.names(diag.table)  <- paste0("Data", 1:6)
colnames(diag.table) <- c("Rhat>1.05", "N. Divergent", "N. Low BFMI", 
                           "Excedeed Treedepth", "Low Bulk ESS", "Low Tail ESS")

diag.table[, 1] <- unlist(
  lapply(fit, function(x) {
    sum(rhat(x, pars = head(paramsample, -1)) > 1.05)
    }))

diag.table[, 2] <- unlist(
  lapply(fit, get_num_divergent)
)

diag.table[, 3] <- unlist(
  lapply(fit, function(x) length(get_low_bfmi_chains(x)))
)

diag.table[, 4] <- unlist(
  lapply(fit, get_num_max_treedepth)
)

diag.table[, 5] <- unlist(
  lapply(diag, function(x) sum(x$Bulk_ESS < 300))
)

diag.table[, 6] <- unlist(
  lapply(diag, function(x) sum(x$Tail_ESS < 300))
)
```

```{r diag_table}
kbl(diag.table, align = "c", booktabs = TRUE)
```

```{r pairs_plot, eval = FALSE}
for (i in 1:4) {
  pairs(fit[[i]], pars = paramplot, log = TRUE, las = 1)
}
rm(i)
```

```{r rhat_plot, fig.height = 8, eval = FALSE}
par(mfrow = c(2, 2))
for (i in 1:4) {
  print(mcmc_rhat(rhat(fit[[i]])))
}
rm(i)
```

```{r trace_plot, eval = FALSE}
for (i in 1:4) {
  print(traceplot(fit[[i]], pars = paramplot, 
                  inc_warmup = FALSE))
}
rm(i)
```

In what follows, different posterior predictive model checking methods are applied on the fitted models in order to illustrate how they work and to explore whether they are useful to identify model misfit. We expect some of this methods to show misfit on Data3 to Data6, as these simulated data sets clearly violate some of the assumptions of the TV-DPCM. In contrast, the fitted models to Data1 and Data2 should not show model misfit given that Data1 perfectly matches the model and Data2 was simulated based on a simplified version of the model.

\newpage

## Sumscores Time Series

A popular test level measure to assess model fit of IRT models is the Test Score distribution [@Li2017; @Zhu2011], which compares the test score distribution of the observed data with the replicated data. However, assessing the test score distribution did not seem to be adequate for the TV-DPCM model as it does not take into account the time component. Because of this, we compared the observed and the replicated time series of the test scores instead. In the next Figure, we plotted the observed test scores against the median test score of the replicated data with its 95\% percentile band of the replicated scores. If a large amount of observed scores are out of the 95\% percentile band, it would indicate some sort of model misfit.   

```{r sums_ts, fig.height = 8}
par(mfrow = c(6, 1), oma = c(1, 3, 1, 1), mar = c(1, 4, 2, 1))
for(i in 1:6) {
  ppmc.sumscore.ts(fit[[i]], standata[[i]])
  mtext(paste0("Data", i), side = 2, line = 4.5, xpd = NA)
}
rm(i)
```

\newpage

## Autocorrelation: 1st, 2nd, and 3rd

One of the key features of the TV-DPCM is the assumption that the latent dynamic process follows a first order TV-AR model. Intuitively, we think that the autocorrelation can be a useful statistic to assess model fit via PPMC methods. Because of this, we computed the autocorrelation of the observed test scores up to lag 3. We expect this statistic to show some kind of misfit if the autoregressive order of the real data is larger than 1. However, this does not seem to be the case given the computed ppp-values presented in the following Table.

```{r acf, results="hide", fig.show = "hide"}
acftable <- matrix(NA, ncol = 3, nrow = 6)
colnames(acftable) <- paste0("ACF", 1:3)
row.names(acftable) <- paste("Data", 1:6)
for(i in 1:6) {
  acftable[i, ] <- ppmc.acf(fit[[i]], standata[[i]], lag.max = 3)
}
rm(i)
```

```{r acftable}
kbl(acftable, align = "c", booktabs = TRUE)
```

\newpage

## Autocorrelation of the Residuals: 1st

Following the same idea of the autocorrelation, we can instead compute the autocorrelation of the residuals after fitting a regular AR model to the sumscores. In other words, we are looking at the partial autocorrelation at lag 2 of the sumscores. The following Figure presents the histogram and the correspond ppp-value of the test statistic for each data set. 

```{r racf, fig.height = 8}
par(mfrow = c(2, 3), mar = c(2, 4, 5, 1))
for(i in 1:6) {
  ppmc.racf(fit[[i]], standata[[i]])
  mtext(paste0("Data", i), side = 3, line = 3.5)
}
rm(i)
```

\newpage

## Partial Autocorrelation given the TV-DPCM

For this statistic, the first partial autocorrelation given the TV-DPCM is computed. This means, that first, the expected scores given the TV-DPCM are computed. From this, we get the residuals of the model. Lastly, the first autocorrelation is computed for the sumscores. The following Figure presents the scatterplots and the corresponding ppp-value of this discrepancy measure for each data set. 

```{r lpacf, fig.height = 8}
par(mfrow = c(2, 3), mar = c(2, 4, 5, 1))
for(i in 1:6) {
  ppmc.lpacf(fit[[i]], standata[[i]], sumscores = TRUE)
  mtext(paste0("Data", i), side = 3, line = 3.5)
}
rm(i)
```

\newpage

## Mean Squared Successive Differences

The Mean Squared Successive Differences (MSSD) is a common statistic used to describe time series. This statistic is a measure of dispersion that takes into account the order of the data. It aims to measure how different are successive observations. The MSSD is defined as follows:

$$
MSSD(X) = \frac{\sum^{n-1}_{i=1}(X_{i+1} - X_{i})^2}{n-1}.
$$

When used as a PPMC method, we would expect that this statistic will indicate misfit, if the time series of the observed and replicated data are too different. The following Figure shows the histogram of the test statistic for each data set.

```{r mssd, fig.height = 8}
par(mfrow = c(2, 3), mar = c(2, 4, 5, 1))
for(i in 1:6) {
  ppmc.mssd(fit[[i]], standata[[i]])
  mtext(paste0("Data", i), side = 3, line = 3.5)
}
rm(i)
```

\newpage

## Items Scores Time Series: Item 1

Similar as with the test scores distribution, the item scores distribution have been used as PPMC methods for IRT models [@Zhu2011]. In this case, we also prefer to compare the observed and replicated time series to take into account the time component. These comparisons are presented in the following for item 5 of each data set.

```{r item_ts, fig.height = 8}
par(mfrow = c(6, 1), oma = c(1, 3, 1, 1), mar = c(1, 4, 2, 1))
for(i in 1:6) {
  ppmc.item.ts(fit[[i]], standata[[i]], quiet = TRUE, items = 5)
  mtext(paste0("Data", i), side = 2, line = 4.5, xpd = NA)
}
rm(i)
```

\newpage

## Item-Total Correlation: Version 1

The item-total correlation is a common statistic used as a PPMC methods for IRT models [@Sinharay2006a; @Zhu2011; @Li2017]. This statistic correlates the scores of an item with the total scores minus the score of the item of interest (rescores). This test statistic allowed identifying model misfit of the Rasch model when the data was simulated with discrimination parameters that varied across items [@Sinharay2006a]. When used to assess model fit of the GRM, it was useful to identify model misfit when the data were multidimensional [@Zhu2011]. Hereby, we try this statistic with the simulated data sets. The following table presents the computed ppp-values for each item and each data set, and the number of items that show misfit.

```{r itcor, fig.show='hide'}
it_cortable <- matrix(NA, ncol = 7, nrow = 6)
colnames(it_cortable) <- c(paste0("Item", 1:6), "N_misfit")
row.names(it_cortable) <- paste("Data", 1:6)
for(i in 1:6) {
  it_cortable[i, 1:6] <- ppmc.itcor(fit[[i]], 
                                 standata[[i]], 
                                 quiet = TRUE, 
                                 method = "pearson")
}
rm(i)

it_cortable[, 7] <- apply(it_cortable[, 1:6], 1, function(x) {
  sum(x < 0.05 | x > 0.95)
})
kbl(it_cortable, align = "c", booktabs = TRUE)
```

## Item-Total Correlation: Version 2

Additionally, we proposed two modified versions of the Item-Total Correlation to account for the time component. In this modified version the idea is that instead of correlating the scores of the item with the rescores, we correlate the them with the residuals of an autoregressive model fitted to the rescores. The ppp-values and the number of misfitting items for each data set are presented in the following table:

```{r itcor2, fig.show='hide'}
it_cor2table <- matrix(NA, ncol = 7, nrow = 6)
colnames(it_cor2table) <- c(paste0("Item", 1:6), "N_misfit")
row.names(it_cor2table) <- paste("Data", 1:6)
for(i in 1:6) {
  it_cor2table[i, 1:6] <- ppmc.itcor2(fit[[i]], 
                                 standata[[i]], 
                                 quiet = TRUE, 
                                 method = "pearson")
}
rm(i)

it_cor2table[, 7] <- apply(it_cor2table[, 1:6], 1, function(x) {
  sum(x < 0.05 | x > 0.95)
})
kbl(it_cor2table, align = "c", booktabs = TRUE)
```


## Item-Total Correlation: Version 3

The second modification follows the same idea of fitting an autoregressive model but in this case the model is fitted to both the item scores and the rescores. Then the residuals of these two models are correlated. For the ppp-values and number of misfitting items given this test statistic, see the following Table:

```{r itcor3, fig.show='hide'}
it_cor3table <- matrix(NA, ncol = 7, nrow = 6)
colnames(it_cor3table) <- c(paste0("Item", 1:6), "N_misfit")
row.names(it_cor3table) <- paste("Data", 1:6)
for(i in 1:6) {
  it_cor3table[i, 1:6] <- ppmc.itcor3(fit[[i]], 
                                 standata[[i]], 
                                 quiet = TRUE)
}
rm(i)

it_cor3table[, 7] <- apply(it_cor3table[, 1:6], 1, function(x) {
  sum(x < 0.05 | x > 0.95)
})
kbl(it_cor3table, align = "c", booktabs = TRUE)
```

\newpage

## Yen's Q1

The Yen's $Q_1$ is an item fit statistic commonly used for IRT models. While it was initially developed for dichotomous items, it was generalized for polytomous items as follows: 

$$
Q_1 - \chi^2 = \sum_{g=1}^{G}\sum_{x=0}^{m_i}N_{g}\frac{(O_{igx} - E_{igx})^2}{E_{igx}},
$$

where individuals are divided into $N$ groups (usually 10) for which the proportion of responses of each response category is computed for the observed distribution and the mean probability to endorse each response category is computed for the expected distribution. The groups are defined by the scores in the latent variable. Hence, the latent scores are ordered to create the groups in such a way that the groups consist of people with similar latent trait scores.

We use this discrepancy measure for the TV-DPCM without any modification. The difference is that the groups do not represent participants with a similar latent trait level but the time points for which the participant had a similar latent state disposition. This discrepancy measure is graphically represented with scatterplots and the ppp-value is computed by counting the proportion of pairs that are above the equality line, see the following Figure that shows the pair of Yen's $Q1$ for the observed and replicated data of item 1 of Data1:

```{r q1_1, fig.height = 5, results='hide'}
ppmc.Q1(fit[[1]], standata[[1]], quiet = TRUE, items = 1)
```

Now, to summarize this discrepancy measure for all items and data sets, the next Table present the corresponding ppp-values and the number of misfitting items according to this discrepancy measure for each data set. 

```{r q1table, fig.show='hide'}
q1table <- matrix(NA, ncol = 7, nrow = 6)
colnames(q1table) <- c(paste0("Item", 1:6), "N_misfit")
row.names(q1table) <- paste("Data", 1:6)
for(i in 1:6) {
  q1table[i, 1:6] <- ppmc.Q1(fit[[i]], standata[[i]], quiet = TRUE)
}
rm(i)

q1table[, 7] <- apply(q1table[, 1:6], 1, function(x) {
  sum(x < 0.05 | x > 0.95)
})
kbl(q1table, align = "c", booktabs = TRUE)
```

\newpage

## Yen's Q1 modified

In this modified version of the Yen's $Q_1$, we define the groups by consecutive observations to take into account the time component. A summary of the ppp-values for each data set is presented in the following Table:

```{r q1alttable, fig.show='hide'}
q1alttable <- matrix(NA, ncol = 7, nrow = 6)
colnames(q1alttable) <- c(paste0("Item", 1:6), "N_misfit")
row.names(q1alttable) <- paste("Data", 1:6)
for(i in 1:6) {
  q1alttable[i, 1:6] <- ppmc.Q1.alt(fit[[i]], standata[[i]], quiet = TRUE)
}
rm(i)

q1alttable[, 7] <- apply(q1alttable[, 1:6], 1, function(x) {
  sum(x < 0.05 | x > 0.95)
})
kbl(q1alttable, align = "c", booktabs = TRUE)
```

\newpage

## Partial Autocorrelation per Item given the TV-DPCM

Here, we compute the first partial autocorrelation per item. As previously, we first compute the expected scores per item given the TV-DPCM, then the residuals per item are computed. Lastly, the autocorrelation of the residual becomes the discrepancy measure.A summary of the ppp-values for each data set is presented in the following Table:

```{r lpacftable, fig.show='hide'}
lpacftable <- matrix(NA, ncol = 7, nrow = 6)
colnames(lpacftable) <- c(paste0("Item", 1:6), "N_misfit")
row.names(lpacftable) <- paste("Data", 1:6)
for(i in 1:6) {
  lpacftable[i, 1:6] <- ppmc.lpacf(fit[[i]], standata[[i]], quiet = TRUE)
}
rm(i)

lpacftable[, 7] <- apply(lpacftable[, 1:6], 1, function(x) {
  sum(x < 0.05 | x > 0.95)
})
kbl(lpacftable, align = "c", booktabs = TRUE)
```

\newpage

## Yen's Q3

The Yen's $Q_3$ is a pair-wise measure that has been effective at identifying violations of unidimensionality with the GRM [@Zhu2011]. This statistics is defined as the correlation between the deviation scores of two items:

$$
Q_{3ij} = Cor(d_i, d_j);
$$

for $i \neq j$, where:

$$
d_{i} = O_i - E_i,
$$

$O_i$ are the observed scores of item $i$ and $E_i$ are the expected scores given the model. The expected scores are computed as follows:

$$
E_i = \sum_{x = 1}^{m_i}xP(X_i = x|\hat{\theta_t}).
$$

In the following plot, we present the ppp-values for each pair of items of each dataset. The ppp-values are represented as pie-charts. If the pie-chart is large, it indicates that the ppp-value is too extreme. 

```{r q3_ppp, results='hide', fig.show='hide'}
sp_plots <- list()
for (i in 1:6) {
  sp_plots[[i]] <- ppmc.Q3(fit[[i]], standata[[i]])$sp
}
```

```{r q3_plots, fig.height = 6, results='hide'}
ggpubr::ggarrange(sp_plots[[1]], sp_plots[[2]], sp_plots[[3]],
                  sp_plots[[4]], sp_plots[[5]], sp_plots[[6]],
                  labels = paste0("Data", 1:6), ncol = 3, nrow = 2,
                  label.x = 0.1)
```

\newpage

## OR 

Another discrepancy measure that is commonly used to assess model misfit of IRT model is the odds ratio [@Sinharay2006a; @Li2017; @Zhu2011]. For polytomous items, one can dichotomize the responses to the items and compute the odds ratio with the dichotomized responses. This is known as the "global" odd ratio and is defined like this:

$$
OR = \frac{n_{11}n_{00}}{n_{10}n_{01}},
$$

where $n_{ij}$ is the number of participants that scored $i$ and $j$ in a pair of items. Thus, the ppp-values of each pair of items for each data set are presented in the following Figure:

```{r OR_ppp, results='hide', fig.show='hide'}
sp_plots <- list()
for (i in 1:6) {
  sp_plots[[i]] <- ppmc.OR(fit[[i]], standata[[i]])$sp
}
```

```{r OR_plots, fig.height = 6, results='hide'}
ggpubr::ggarrange(sp_plots[[1]], sp_plots[[2]], sp_plots[[3]],
                  sp_plots[[4]], sp_plots[[5]], sp_plots[[6]],
                  labels = paste0("Data", 1:6), ncol = 3, nrow = 2,
                  label.x = 0.1)
```


\newpage

## OR Modified

Here, we present a modified discrepancy measure of the odds ratio. The idea is that the odd ratio are computed for each half of the time series and then the difference between these these odds ratio is used as the discrepancy measure. We expect this measure to be useful to test trend stationarity and longitudinal measurement invariance. The correspoding ppp-values for each pair of items ar presented in the following plot. 

```{r ORdiff_ppp, results='hide', fig.show='hide'}
sp_plots <- list()
for (i in 1:6) {
  sp_plots[[i]] <- ppmc.ORDiff(fit[[i]], standata[[i]])$sp
}
```

```{r ORdiff_plots, fig.height = 6, results='hide'}
ggpubr::ggarrange(sp_plots[[1]], sp_plots[[2]], sp_plots[[3]],
                  sp_plots[[4]], sp_plots[[5]], sp_plots[[6]],
                  labels = paste0("Data", 1:6), ncol = 3, nrow = 2, 
                  label.x = 0.1)
```

\newpage

## RESID

The absolute item covariance residual (RESID) was useful to identify violations of unidimensionality and local dependence of the GRM [@Zhu2011]. This statistic is defined as the absolute value of the difference between the observed and the expected item covariances.

$$
RESID_{ij} = |cov(X_{i}, X_{j}) - cov(E_{i}, E_{j})|
$$

See in the following Figure, the ppp-values for each pair fo items and each simulated data set. 

```{r resid_ppp, results='hide', fig.show='hide'}
sp_plots <- list()
for (i in 1:6) {
  sp_plots[[i]] <- ppmc.cov.resid(fit[[i]], standata[[i]])$sp
}
```

```{r resid_plots, fig.height = 6, results='hide'}
ggpubr::ggarrange(sp_plots[[1]], sp_plots[[2]], sp_plots[[3]],
                  sp_plots[[4]], sp_plots[[5]], sp_plots[[6]],
                  labels = paste0("Data", 1:6), ncol = 3, nrow = 2, 
                  label.x = 0.1)
```

\newpage

## RESID Diff

Lastly, similarly as we suggested with the odds ratio, we also suggest to compute the absolute item covariance residual of each half of the time series and compute the difference. Then, use the difference as the discrepancy measure.  By doing this, we obtained the following ppp-values for each data set:

```{r rediff_ppp, results='hide', fig.show='hide'}
sp_plots <- list()
for (i in 1:6) {
  sp_plots[[i]] <- ppmc.cov.rediff(fit[[i]], standata[[i]])$sp
}
```

```{r rediff_plots, fig.height = 6, results='hide'}
ggpubr::ggarrange(sp_plots[[1]], sp_plots[[2]], sp_plots[[3]],
                  sp_plots[[4]], sp_plots[[5]], sp_plots[[6]],
                  labels = paste0("Data", 1:6), ncol = 3, nrow = 2, 
                  label.x = 0.1)
```

# Conclusion

In the present document, we illustrate the ideas that we have about the PPMC methods and some modified versions of these methods that can be used to assess the fit of the TV-DPCM. In this example, we simulated 6 different data sets, 4 of which violated some of the assumptions of the model. Then, we fitted the TV-DPCM model to each data set and we computed the PPMC methods available. From these preliminary analyses, it seems that the pair-wise discrepancy measures such as Yen's $Q_3$ and the odds ratio are to some extend useful to identify violations of unidimensionality. The other PPMC methods presented did not seem to be able to identify model misfit with the given data sets.

```{r clean}
rm(list = setdiff(ls(), c(lsf.str(), "i", "seeds", "setmodel", "params")))
```

# References
