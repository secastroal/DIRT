---
title: "Testing Posterior Predictive Model Checking for the Time-Varying Dynamic Partial Credit Model"
author: "Sebastian Castro-Alvarez"
date: "`r format(Sys.Date(), '%B %d del %Y')`"
header-includes:
  - \usepackage{booktabs}
  - \usepackage{longtable}
  - \usepackage{array}
  - \usepackage{multirow}
  - \usepackage{float}
  - \usepackage{amsmath}
output: 
  pdf_document:
    toc: false
bibliography: references.bib
csl: apa7.csl
link-citations: true
always_allow_html: true
---

```{r setup, include=FALSE}
library(knitr)
library(kableExtra)
knitr::opts_chunk$set(echo = FALSE, fig.height = 5, fig.width = 8,  
                      warning=FALSE, message=FALSE)

library(rstan)
rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())
library(bayesplot)
library("splines")

source("../R/IRT_models.R")
source("../R/IRT_plots.R")
source("../R/PPC.R")
```

```{r seed}
seed <- 1001
set.seed(seed)
```

```{r setmodel}
# Use TVPCM, ARPCM, or PCM
setmodel <- "TVPCM"
# render("Rmarkdown/PPMC_Tests.Rmd", output_file = paste0("Rmarkdown/PPMC_Tests_", setmodel, "_", seed))
```

In this document, we test the posterior predictive model checking methods that we are developing for the time-varying dynamic partial credit model (TV-DPCM). For this, we simulated data based on the TV-DPCM as well as data that violates some of the assumptions of the model.

# TV-DPCM

The time-varying dynamic partial credit model is a model that aims to analyze the psychological time series of one individual. Consider for example that a person that is attending therapy is requested to report their emotions multiple times on a daily basis to monitor their progress. Thus, the person has to answer multiple likert-scale questions (the same questions every time) about how they feel. Moreover, we assume that some of these emotions measure a construct such as positive or negative affect. Given this situation, the observed data are categorical time series from a single individual. To analyze such data, we are developing (work still in progress) the time-varying dynamic partial credit model. 

This model combines two frameworks: The item response theory [@Embretson2000] and the time-varying vector autoregressive model [@Bringmann2017]. In particular, the measurement component of the model uses the partial credit model [@Masters2016] to relate the responses to the items with the latent construct of interest. Then, at the latent label, the latent variable, which represents the latent state dispositions of the individual at each time point, is modeled by means of a time-varying autoregressive model [@Bringmann2017]. In this case, we only allow the intercept of the autoregressive model to vary over time while the autoregressive effect is assumed to be time invariant. Thus, the intercept is further modeled with the generalized additive model with splines. This allows the time series to follow a non linear (smooth) trend.

# Data Simulation

In this example, we simulated four data sets. The first data set perfectly matches the proposed TV-DPCM. The second data set is a constrained version of the TV-DPCM model in which the intercept does not vary over time. A third data set assumes that the latent structure is bidimensional instead of unidimensional. The last data set assumes that there is item parameter drift (i.e., the item parameters change over time). For these data sets, we simulated 200 time points of 6 items each with 5 response options. Moreover, the autoregressive effect is set at 0.5 and the variance of the innovations is set to 1.  

```{r fixvalues}
nT     <- 200   # Number of time points
I      <- 6     # Number of items
K      <- 5     # Number of categories per item
M      <- K - 1 # Number of thresholds per item
lambda <- 0.5   # Size of the autoregressive effect
in_var <- 1     # Variance of the innovations
```

Data1 perfectly matches the TV-DPCM. In this case, we simulate the time-varying intercept based on a B-splines of degree three with 2 internal knots. The simulated time series of the sumscores are presented in following figure: 

```{r data1}
time     <- 1:nT
n_knots  <- 4  # Define number of knots.
s_degree <- 3  # Define degree of splines.
n_basis  <- n_knots + s_degree - 1 # Compute number of basis.

# Create the b-splines within the time interval.
B_true <- t(bs(time, df = n_basis, degree = s_degree, intercept = TRUE))

# Define intercept and coefficients to generate the time-varying intercept based 
# on the b-splines.
a0 <- 0                    # intercept
a  <- rnorm(n_basis, 0, 1) # coefficients of B-splines

# Compute time-varying intercept based on time and b-splines. 
tv_int <- as.vector(a0 * time + a %*% B_true)

# Repeat lambda
tv_lambda <- rep(lambda, nT)

# Generate the latent scores theta
theta <- rep(NA, nT)

# First theta based on an stationary marginal distribution (see Bringmann et al., 2017).
theta[1] <- rnorm(1, tv_int[1]/(1 - tv_lambda[1]), sqrt(in_var/(1 - tv_lambda[1] ^ 2)))

for (t in 2:nT) {
  theta[t] <- tv_int[t] + tv_lambda[t] * theta[t - 1] + rnorm(1, 0, sqrt(in_var))
}
rm(t)

theta_splines <- theta

attractor <- tv_int / (1 - lambda)
p_var     <- in_var / (1 - lambda ^ 2)

# Next, we generate data based on the PCM and the thetas we just created.

# Create item parameters

# Thresholds
thresholds <- t(apply(matrix(runif(I * M, .3, 1), I), 1, cumsum))
if (M == 1) {thresholds <- t(thresholds)}
thresholds <- thresholds - rowMeans(thresholds)
thresholds <- thresholds + rnorm(I)

# Location
delta <- rowMeans(thresholds)

# Step parameters
taus <- thresholds - delta

# Generate responses
probs.array <- array(NA, dim = c(length(theta), I, K))

for (y in 0:M) {
  probs.array[, , y + 1] <- P.GPCM(y     = y, 
                                   alpha = rep(1, I), 
                                   delta = delta, 
                                   taus  = taus, 
                                   theta = theta, 
                                   M     = M)
}
responses   <- apply(probs.array, 1:2, function(vec) {which( rmultinom(1, 1, vec) == 1) - 1 })
responses   <- responses + 1 # To fit the PCM in stan, items should be coded starting from 1. 
rm(probs.array, y)

data1 <- responses
```

```{r data1_plot}
plot(rowSums(data1), type = "l", ylab = "sumscores", xlab = "Time",
     las = 1, lwd = 2)
```

Secondly, Data2 was simulated based on a simpler version of the model that does not allow the intercept to vary over time. In this case, the latent variable is also modeled after an autoregressive model, but the structure is assumed to be stationary. Therefore, there are not trends in the time series. In this case, the simulated time series of the sumscores look like this:

```{r data2}

theta <- rep(NA, nT)

theta[1] <- rnorm(1)

for (i in 2:nT) {
  theta[i] <- lambda * theta[i - 1] + rnorm(1, 0, 1) 
}
rm(i)

# Generate responses
probs.array <- array(NA, dim = c(length(theta), I, K))

for (y in 0:M) {
  probs.array[, , y + 1] <- P.GPCM(y     = y, 
                                   alpha = rep(1, I), 
                                   delta = delta, 
                                   taus  = taus, 
                                   theta = theta, 
                                   M     = M)
}
responses   <- apply(probs.array, 1:2, function(vec) {which( rmultinom(1, 1, vec) == 1) - 1 })
responses   <- responses + 1 # To fit the PCM in stan, items should be coded starting from 1. 
rm(probs.array, y)

data2 <- responses

```

```{r data2_plot}
plot(rowSums(data2), type = "l", ylab = "sumscores", xlab = "Time",
     las = 1, lwd = 2)
```

The next data set (Data3) assumes that the factor structure is bidimensional instead of unidimensional. The two factors are correlated ($r = 0.6$). The latent process of these two factors was also simulated based on an autoregressive model without time varying parameters. Thus, the simulated time series of the sumscores of each factor and the whole scale are presented in the following figure:

```{r data3}
# Now, theta is a matrix with two factors that are weakly correlated.
theta <- matrix(NA, nrow = nT, ncol = 2)
mu    <- c(0, 0)
Sigma <- matrix(c(1, 0.3, 0.3, 1), 2)

theta[1, ] <- MASS::mvrnorm(1, mu = mu, Sigma = Sigma)

for (i in 2:nT) {
  theta[i, ] <- lambda * theta[i - 1, ] + MASS::mvrnorm(1, mu = mu, Sigma = Sigma) 
}
rm(i, mu, Sigma)

# Now, let's use the same item parameters to generate the data but using three
# items for one factor and three items for the other factor.

# Responses factor 1
probs.array <- array(NA, dim = c(dim(theta)[1], I / 2, K))

for (y in 0:M) {
  probs.array[, , y + 1] <- P.GPCM(y     = y, 
                                   alpha = rep(1, I / 2), 
                                   delta = delta[1:3], 
                                   taus  = taus[1:3, ], 
                                   theta = theta[, 1], 
                                   M     = M)
}
responses1   <- apply(probs.array, 1:2, function(vec) {which( rmultinom(1, 1, vec) == 1) - 1 })
responses1   <- responses1 + 1 # To fit the PCM in stan, items should be coded starting from 1. 
rm(probs.array, y)

# Responses factor 2
probs.array <- array(NA, dim = c(dim(theta)[1], I / 2, K))

for (y in 0:M) {
  probs.array[, , y + 1] <- P.GPCM(y     = y, 
                                   alpha = rep(1, I / 2), 
                                   delta = delta[4:6], 
                                   taus  = taus[4:6, ], 
                                   theta = theta[, 2], 
                                   M     = M)
}
responses2   <- apply(probs.array, 1:2, function(vec) {which( rmultinom(1, 1, vec) == 1) - 1 })
responses2   <- responses2 + 1 # To fit the PCM in stan, items should be coded starting from 1. 
rm(probs.array, y)

responses <- cbind(responses1, responses2)
rm(responses1, responses2)

data3 <- responses
```

```{r data3_plot}
plot(rowSums(data3), type = "l", ylab = "sumscores", xlab = "Time",
     las = 1, ylim = c(2.75, 30.25), lwd = 2)
lines(rowSums(data3[, 1:3]), col = "blue", lwd = 2)
lines(rowSums(data3[, 4:6]), col = "darkgreen", lwd = 2)
legend("topright", c("Total", "Factor 1", "Factor 2"),
       col = c("black", "blue", "darkgreen"), lty = 1, lwd = 2)

```

Lastly, Data4 assumes that the item parameters change over time. For this, we keep the same generated thetas as the ones use to generate Data1. We also use the same item parameters as we used for the other data sets to generate the first half of the responses. In contrast, for the second half of the responses, the thresholds parameters are modified by resting 2 to all of them. This means that the individual is more likely to endorse high responses on the second half of the measurement. Now, the following figure presents the simulated time series:

```{r data4}
# Use same theta as in data1
theta <- theta_splines

# Create item parameters for the second half.
thresholds2 <- thresholds - 2

# Location
delta2 <- rowMeans(thresholds2)

# Step parameters
taus2 <- thresholds2 - delta2

# Generate responses  half 1
probs.array <- array(NA, dim = c(length(theta)/2, I, K))

for (y in 0:M) {
  probs.array[, , y + 1] <- P.GPCM(y     = y, 
                                   alpha = rep(1, I), 
                                   delta = delta, 
                                   taus  = taus, 
                                   theta = theta[1:100], 
                                   M     = M)
}
responses1   <- apply(probs.array, 1:2, function(vec) {which( rmultinom(1, 1, vec) == 1) - 1 })
responses1   <- responses1 + 1 # To fit the PCM in stan, items should be coded starting from 1. 
rm(probs.array, y)

# Generate responses  half 2
probs.array <- array(NA, dim = c(length(theta)/2, I, K))

for (y in 0:M) {
  probs.array[, , y + 1] <- P.GPCM(y     = y, 
                                   alpha = rep(1, I), 
                                   delta = delta2, 
                                   taus  = taus2, 
                                   theta = theta[101:200], 
                                   M     = M)
}
responses2   <- apply(probs.array, 1:2, function(vec) {which( rmultinom(1, 1, vec) == 1) - 1 })
responses2   <- responses2 + 1 # To fit the PCM in stan, items should be coded starting from 1. 
rm(probs.array, y)

responses <- rbind(responses1, responses2)
#rm(responses1, responses2)

data4 <- responses
```

```{r data4_plot}
plot(rowSums(data4[1:100, ]), type = "l", ylab = "sumscores", xlab = "Time",
     las = 1, xlim = c(1, 200), lwd = 2, col = "blue")
lines(100:200, rowSums(data4[100:200, ]), col = "darkgreen", lwd = 2)
```

# Posterior Predictive Model Checking Methods

In this section, we fit the TV-DPCM to all the generated data sets. Then, we compute some of the posterior predictive checking methods and modified versions of these methods that have been proposed for IRT in @Li2017, @Sinharay2006a, and @Zhu2011. First, let's verify by means of the Rubin-Gelman statistics that the model converged on each of the data sets. This is presented in the following figure: 

```{r selectmodel}
if (setmodel == "TVPCM") {
  modelfile   <- "tv_dpcm_int.stan"
  paramsample <- c("beta", "theta", "lambda", "sigma2", "p_var", 
                   "attractor", "rep_y")
  paramplot   <- c("beta[1,1]", "beta[3,3]", "beta[6,4]", "theta[1]", 
                   "theta[100]", "theta[200]", "lambda", "sigma2", 
                   "p_var")
  inits <- function() {
  list(#a_raw  = rnorm(n_basis),
       #a0     = rnorm(1),
       #tau    = rnorm(1),
       lambda = runif(1, -1, 1),
       beta   = array(rnorm(I * (K - 1), 0, 3), dim = c(I, K - 1)),
       inno   = rnorm(nT, 0, 3),
       sigma2 = rlnorm(1, 1))
  }
}

if (setmodel == "ARPCM") {
  modelfile   <- "ar_irt_pcm_na_free.stan"
  paramsample <- c("beta", "theta", "lambda", "sigma2", "rep_y")
  paramplot   <- c("beta[1,1]", "beta[3,3]", "beta[6,4]", "theta[1]", 
                   "theta[100]", "theta[200]", "lambda", "sigma2")
  inits <- function() {
  list(lambda = runif(1, -1, 1),
       beta   = array(rnorm(I * (K - 1), 0, 3), dim = c(I, K - 1)),
       inno   = rnorm(nT, 0, 3),
       sigma2 = rlnorm(1, 1))
  }
}

if (setmodel == "PCM") {
  modelfile   <- "irt_pcm_long.stan"
  paramsample <- c("beta", "theta", "rep_y")
  paramplot   <- c("beta[1,1]", "beta[3,3]", "beta[6,4]", "theta[1]", 
                   "theta[100]", "theta[200]")
  inits <- function() {
  list(beta   = array(rnorm(I * (K - 1), 0, 3), dim = c(I, K - 1)),
       theta   = rnorm(nT, 0, 3))
  }
}

```


```{r stanfit}
model <- stan_model(file = paste0("../Stan/", modelfile), verbose = FALSE)

knots <- trunc(seq(1, nT, length.out = n_knots * 2))

datasets <- list(data1, data2, data3, data4)
standata <- list()
fit      <- list()

# Fit the TV-DPCM model with stan
for (i in 1:4) {

responses <- datasets[[i]]

standata[[i]] <- list(nT = nT,
                      n_knots  = n_knots * 2,
                      knots    = knots,
                      s_degree = s_degree,
                      I  = I,
                      K  = K,
                      N  = nT * I,
                      N_obs = sum(!is.na(c(responses))),
                      tt = rep(1:nT, I),
                      ii = rep(1:I, each = nT),
                      tt_obs = rep(1:nT, I)[!is.na(c(responses))],
                      ii_obs = rep(1:I, each = nT)[!is.na(c(responses))],
                      y_obs  = c(responses)[!is.na(c(responses))],
                      time = time)
            
begin.time <- proc.time()
fit[[i]] <- sampling(model,                            # Stan model. 
                     data = standata[[i]],             # Data.
                     iter = 2000,                      # Number of iterations.
                     chains  = 3,                      # Number of chains.
                     warmup  = 1000,                   # Burn-in samples.
                     init    = inits,
                     pars    = paramsample,
                     control = list(adapt_delta=0.8, max_treedepth = 10)) # Other parameters to control sampling behavior.
run.time <- proc.time() - begin.time
rm(begin.time)
            
}
```

## Verify model convergence.

```{r diagnostics, results='hide'}

diag <- lapply(fit, function (x) {
  monitor(extract(x, permuted = FALSE, inc_warmup = FALSE), warmup = 0)
})

diag.table <- matrix(NA, nrow = 6, ncol = 4)
colnames(diag.table)  <- paste0("Data", 1:4)
row.names(diag.table) <- c("Rhat>1.05", "N. Divergent", "N. Low BFMI", 
                           "Excedeed Treedepth", "Low Bulk ESS", "Low Tail ESS")

diag.table[1, ] <- unlist(
  lapply(fit, function(x) {
    sum(rhat(x, pars = head(paramsample, -1)) > 1.05)
    }))

diag.table[2, ] <- unlist(
  lapply(fit, get_num_divergent)
)

diag.table[3, ] <- unlist(
  lapply(fit, function(x) length(get_low_bfmi_chains(x)))
)

diag.table[4, ] <- unlist(
  lapply(fit, get_num_max_treedepth)
)

diag.table[5, ] <- unlist(
  lapply(diag, function(x) sum(x$Bulk_ESS < 300))
)

diag.table[6, ] <- unlist(
  lapply(diag, function(x) sum(x$Tail_ESS < 300))
)
```

```{r diag_table}
kbl(diag.table, align = "c", booktabs = TRUE)
```

```{r pairs_plot}
for (i in 1:4) {
  pairs(fit[[i]], pars = paramplot, log = TRUE, las = 1)
}
rm(i)
```

```{r rhat_plot, fig.height = 8}
par(mfrow = c(2, 2))
for (i in 1:4) {
  print(mcmc_rhat(rhat(fit[[i]])))
}
rm(i)
```
```{r trace_plot}
for (i in 1:4) {
  print(traceplot(fit[[i]], pars = paramplot, 
                  inc_warmup = FALSE))
}
rm(i)
```

## Sumscores Time Series

```{r sums_ts, fig.height = 8}
par(mfrow = c(4, 1))
for(i in 1:4) {
  ppc.sumscore.ts(fit[[i]], standata[[i]])
}
rm(i)
```

## Autocorrelation: 1st, 2nd, and 3rd

```{r acf, fig.height = 8}
par(mfrow = c(4, 3))
for(i in 1:4) {
  ppc.acf(fit[[i]], standata[[i]], lag.max = 3)
}
rm(i)
```

## Autocorrelation of the Residuals: 1st

```{r racf, fig.height = 8}
par(mfrow = c(2, 2))
for(i in 1:4) {
  ppc.racf(fit[[i]], standata[[i]])
}
rm(i)
```

## Items Scores Time Series: Item 1

```{r item_ts, fig.height = 8}
par(mfrow = c(4, 1))
for(i in 1:4) {
  ppc.item.ts(fit[[i]], standata[[i]], quiet = TRUE, items = 1)
}
rm(i)
```

## Item-Total Correlation: Version 1
Data1:
```{r itcor_1, fig.height = 6, results='hide'}
par(mfrow = c(2, 3))
ppc.itcor(fit[[1]], standata[[1]], quiet = TRUE, method = "pearson")
```
Data2:
```{r itcor_2, fig.height = 6, results='hide'}
par(mfrow = c(2, 3))
ppc.itcor(fit[[2]], standata[[2]], quiet = TRUE, method = "pearson")
```
Data3:
```{r itcor_3, fig.height = 6, results='hide'}
par(mfrow = c(2, 3))
ppc.itcor(fit[[3]], standata[[3]], quiet = TRUE, method = "pearson")
```
Data4:
```{r itcor_4, fig.height = 6, results='hide'}
par(mfrow = c(2, 3))
ppc.itcor(fit[[4]], standata[[4]], quiet = TRUE, method = "pearson")
```

## Item-Total Correlation: Version 2
Data1:
```{r itcor2_1, fig.height = 6, results='hide'}
par(mfrow = c(2, 3))
ppc.itcor2(fit[[1]], standata[[1]], quiet = TRUE, method = "polyserial")
```
Data2:
```{r itcor2_2, fig.height = 6, results='hide'}
par(mfrow = c(2, 3))
ppc.itcor2(fit[[2]], standata[[2]], quiet = TRUE, method = "polyserial")
```
Data3:
```{r itcor2_3, fig.height = 6, results='hide'}
par(mfrow = c(2, 3))
ppc.itcor2(fit[[3]], standata[[3]], quiet = TRUE, method = "polyserial")
```
Data4:
```{r itcor2_4, fig.height = 6, results='hide'}
par(mfrow = c(2, 3))
ppc.itcor2(fit[[4]], standata[[4]], quiet = TRUE, method = "polyserial")
```


## Item-Total Correlation: Version 3
Data1:
```{r itcor3_1, fig.height = 6, results='hide'}
par(mfrow = c(2, 3))
ppc.itcor3(fit[[1]], standata[[1]], quiet = TRUE)
```
Data2:
```{r itcor3_2, fig.height = 6, results='hide'}
par(mfrow = c(2, 3))
ppc.itcor3(fit[[2]], standata[[2]], quiet = TRUE)
```
Data3:
```{r itcor3_3, fig.height = 6, results='hide'}
par(mfrow = c(2, 3))
ppc.itcor3(fit[[3]], standata[[3]], quiet = TRUE)
```
Data4:
```{r itcor3_4, fig.height = 6, results='hide'}
par(mfrow = c(2, 3))
ppc.itcor3(fit[[4]], standata[[4]], quiet = TRUE)
```


# References
